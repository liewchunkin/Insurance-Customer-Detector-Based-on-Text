{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap Using snscrape (cli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pip install command below if you don't already have the library\n",
    "# !pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "# Run the below command if you don't already have Pandas\n",
    "# !pip install pandas\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap sport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables to be used in format string command below\n",
    "tweet_count = 5000\n",
    "text_query = \"sport\"\n",
    "since_date = \"1967-11-01\"\n",
    "until_date = \"2022-11-01\"\n",
    "\n",
    "# Using OS library to call CLI commands in Python\n",
    "os.system('snscrape --jsonl --max-results {} --since {} twitter-search \"{} until:{}\"> sport.json'.format(tweet_count, since_date, text_query, until_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "tweets_df1 = pd.read_json('sport.json', lines=True)\n",
    "\n",
    "# Displays first 5 entries from dataframe\n",
    "tweets_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "tweets_df1.to_csv('sports.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap food data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables to be used in format string command below\n",
    "tweet_count = 5000\n",
    "text_query = \"food\"\n",
    "since_date = \"1967-11-01\"\n",
    "until_date = \"2022-11-01\"\n",
    "\n",
    "# Using OS library to call CLI commands in Python\n",
    "os.system('snscrape --jsonl --max-results {} --since {} twitter-search \"{} until:{}\"> food.json'.format(tweet_count, since_date, text_query, until_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "tweets_df2 = pd.read_json('food.json', lines=True)\n",
    "\n",
    "# Displays first 5 entries from dataframe\n",
    "tweets_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "tweets_df2.to_csv('food.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap car data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables to be used in format string command below\n",
    "tweet_count = 5000\n",
    "text_query = \"car\"\n",
    "since_date = \"2021-11-01\"\n",
    "until_date = \"2022-11-01\"\n",
    "\n",
    "# Using OS library to call CLI commands in Python\n",
    "os.system('snscrape --jsonl --max-results {} --since {} twitter-search \"{} until:{}\"> car.json'.format(tweet_count, since_date, text_query, until_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "tweets_df3 = pd.read_json('car.json', lines=True)\n",
    "\n",
    "# Displays first 5 entries from dataframe\n",
    "tweets_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "tweets_df3.to_csv('car.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the dataset from the CSV and save it to 'data_text'\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('car.csv', error_bad_lines=False);\n",
    "\n",
    "# We only need the Headlines text column from the data\n",
    "data_text = data[:5000][['Text']];\n",
    "\n",
    "data_text['index'] = data_text.index\n",
    "\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the dataset from the CSV and save it to 'data_text1'\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv('food.csv',\n",
    "                 lineterminator='\\n')\n",
    "\n",
    "# We only need the Headlines text column from the data\n",
    "data_text1 = data1[:5000][['Text']];\n",
    "\n",
    "\n",
    "\n",
    "data_text1['index'] = data_text1.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the dataset from the CSV and save it to 'data_text2'\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "data2 = pd.read_csv('sports.csv',\n",
    "                 lineterminator='\\n')\n",
    "\n",
    "# We only need the Headlines text column from the data\n",
    "data_text2 = data2[:5000][['Text']];\n",
    "\n",
    "frames = [data_text, data_text1, data_text2]\n",
    "\n",
    "data_text2['index'] = data_text2.index\n",
    "\n",
    "documents = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get the total number of documents\n",
    "'''\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My '06 Chevy Silverado 1500HD with 6.0 liter ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have owned 5 Silverado's since 1999, would ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am a line driver for a local trucking compa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We purchased this thruck to pull a 33 ft Amer...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This has been the best truck I've ever owned....</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  index\n",
       "0   My '06 Chevy Silverado 1500HD with 6.0 liter ...      0\n",
       "1   I have owned 5 Silverado's since 1999, would ...      1\n",
       "2   I am a line driver for a local trucking compa...      2\n",
       "3   We purchased this thruck to pull a 33 ft Amer...      3\n",
       "4   This has been the best truck I've ever owned....      4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing ##\n",
    "For example, tokenization, stopwords removal, lemmatized and stemmed words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "# !pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        \n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            \n",
    "            # TODO: Apply lemmatize_stemming() on the token, then add to the results list\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['', 'Corvette', 'performance', 'with', 'a', 'cavaliers', '\\rquality.', '', 'a/c', 'went', 'out', 'at', '37K', 'miles,', '', '\\rthanks', 'GM,', '', 'power', 'windows', 'roll', 'up', 'and', '\\rdown', 'like', 'molasses.', '', '', 'motors', 'replaced', '\\rtwice.', '', '', 'ttops', 'are', 'starting', 'to', 'leak.', '', '\\rradio', 'speakers', 'blew.', '\\r', 'handles', 'like', 'a', 'dream,', '', 'fast', 'as', 'hell.', '', '\\rbrakes', 'stop', 'on', 'a', 'dime.\\r', 'i', 'love', 'this', 'car', 'lol']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['corvett', 'perform', 'cavali', 'qualiti', 'go', 'mile', 'thank', 'power', 'window', 'roll', 'like', 'molass', 'motor', 'replac', 'twice', 'ttop', 'start', 'leak', 'radio', 'speaker', 'blow', 'handl', 'like', 'dream', 'fast', 'hell', 'brake', 'stop', 'dime', 'love']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview a document after preprocessing\n",
    "'''\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "document_num = 4310\n",
    "doc_sample = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "\n",
    "words = []\n",
    "\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "    \n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My '06 Chevy Silverado 1500HD with 6.0 liter ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have owned 5 Silverado's since 1999, would ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am a line driver for a local trucking compa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We purchased this thruck to pull a 33 ft Amer...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This has been the best truck I've ever owned....</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Smaller than I expected which is my fault but ...</td>\n",
       "      <td>4995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Bought this for my 5 year old grandson and he ...</td>\n",
       "      <td>4996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Excellent. Very good material.</td>\n",
       "      <td>4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>this product made me a daredevil. removed my f...</td>\n",
       "      <td>4998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>used for kids bday and all of the kiddos loved...</td>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  index\n",
       "0      My '06 Chevy Silverado 1500HD with 6.0 liter ...      0\n",
       "1      I have owned 5 Silverado's since 1999, would ...      1\n",
       "2      I am a line driver for a local trucking compa...      2\n",
       "3      We purchased this thruck to pull a 33 ft Amer...      3\n",
       "4      This has been the best truck I've ever owned....      4\n",
       "...                                                 ...    ...\n",
       "4995  Smaller than I expected which is my fault but ...   4995\n",
       "4996  Bought this for my 5 year old grandson and he ...   4996\n",
       "4997                     Excellent. Very good material.   4997\n",
       "4998  this product made me a daredevil. removed my f...   4998\n",
       "4999  used for kids bday and all of the kiddos loved...   4999\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess all the text\n",
    "\n",
    "**Note**: This may take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess all the texts, saving the list of results as 'processed_docs'\n",
    "'''\n",
    "processed_docs = documents['Text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [chevi, silverado, liter, awesom, truck, wheel...\n",
       "1    [own, silverado, consid, truck, allison, trans...\n",
       "2    [line, driver, local, truck, compani, yard, wa...\n",
       "3    [purchas, thruck, pull, americamp, trailer, pl...\n",
       "4    [best, truck, own, alaska, florida, tow, trave...\n",
       "5    [purchas, truck, truck, year, camper, haulin, ...\n",
       "6    [buy, truck, februari, year, mile, mile, run, ...\n",
       "7    [haul, pallet, pave, stone, sweat, tow, packag...\n",
       "8    [recent, purchas, truck, know, test, drive, go...\n",
       "9    [buy, truck, septemb, tow, travel, trailer, pl...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Preview 'processed_docs'\n",
    "'''\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 awesom\n",
      "1 camper\n",
      "2 chevi\n",
      "3 countri\n",
      "4 economi\n",
      "5 fall\n",
      "6 fuel\n",
      "7 liter\n",
      "8 long\n",
      "9 mile\n",
      "10 mileag\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "\n",
    "for k, v in dictionary.iteritems():\n",
    "    \n",
    "    print(k, v)\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gensim filter_extremes **\n",
    "\n",
    "[`filter_extremes(no_below=5, no_above=0.5, keep_n=100000)`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes)\n",
    "\n",
    "Filter out tokens that appear in\n",
    "\n",
    "* less than no_below documents (absolute number) or\n",
    "* more than no_above documents (fraction of total corpus size, not absolute number).\n",
    "* after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 5 times\n",
    "- words appearing in more than 50% of all documents\n",
    "'''\n",
    "# TODO: apply dictionary.filter_extremes() with the parameters mentioned above\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gensim doc2bow **\n",
    "\n",
    "[`doc2bow(document)`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow)\n",
    "\n",
    "* Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 1),\n",
       " (22, 1),\n",
       " (36, 1),\n",
       " (43, 2),\n",
       " (47, 1),\n",
       " (63, 1),\n",
       " (84, 1),\n",
       " (97, 1),\n",
       " (101, 1),\n",
       " (117, 1),\n",
       " (124, 1),\n",
       " (165, 1),\n",
       " (189, 1),\n",
       " (224, 1),\n",
       " (262, 1),\n",
       " (382, 1),\n",
       " (409, 1),\n",
       " (464, 1),\n",
       " (513, 1),\n",
       " (607, 1),\n",
       " (741, 1),\n",
       " (754, 1),\n",
       " (815, 1),\n",
       " (994, 1),\n",
       " (1180, 1),\n",
       " (1704, 1),\n",
       " (2796, 1),\n",
       " (3480, 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Checking Bag of Words corpus for our sample document --> (token_id, token_count)\n",
    "'''\n",
    "bow_corpus[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 9 (\"mile\") appears 1 time.\n",
      "Word 22 (\"power\") appears 1 time.\n",
      "Word 36 (\"dream\") appears 1 time.\n",
      "Word 43 (\"like\") appears 2 time.\n",
      "Word 47 (\"love\") appears 1 time.\n",
      "Word 63 (\"perform\") appears 1 time.\n",
      "Word 84 (\"start\") appears 1 time.\n",
      "Word 97 (\"motor\") appears 1 time.\n",
      "Word 101 (\"replac\") appears 1 time.\n",
      "Word 117 (\"hell\") appears 1 time.\n",
      "Word 124 (\"thank\") appears 1 time.\n",
      "Word 165 (\"go\") appears 1 time.\n",
      "Word 189 (\"brake\") appears 1 time.\n",
      "Word 224 (\"handl\") appears 1 time.\n",
      "Word 262 (\"blow\") appears 1 time.\n",
      "Word 382 (\"fast\") appears 1 time.\n",
      "Word 409 (\"leak\") appears 1 time.\n",
      "Word 464 (\"corvett\") appears 1 time.\n",
      "Word 513 (\"twice\") appears 1 time.\n",
      "Word 607 (\"qualiti\") appears 1 time.\n",
      "Word 741 (\"stop\") appears 1 time.\n",
      "Word 754 (\"dime\") appears 1 time.\n",
      "Word 815 (\"radio\") appears 1 time.\n",
      "Word 994 (\"roll\") appears 1 time.\n",
      "Word 1180 (\"window\") appears 1 time.\n",
      "Word 1704 (\"speaker\") appears 1 time.\n",
      "Word 2796 (\"cavali\") appears 1 time.\n",
      "Word 3480 (\"molass\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview Bag-of-words for our sample preprocessed document\n",
    "'''\n",
    "# Here document_num is document number 4310 which we have checked in Step 2\n",
    "bow_doc_4310 = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    \n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: TF-IDF on our document set ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel<num_docs=15000, num_nnz=327505>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create tf-idf model object using models.TfidfModel on 'bow_corpus' and save it to 'tfidf'\n",
    "'''\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.138133469731731), (11, 0.23409870433971797), (13, 0.44015955521596106), (14, 0.1395938935085208), (16, 0.584844002494709), (17, 0.15750453475695322), (18, 0.26891911964944715), (19, 0.1703065596103802), (20, 0.11740536660872368), (21, 0.11665888454858028), (22, 0.10679116858701652), (23, 0.08931462024018134), (24, 0.14249660210251763), (25, 0.1456266410619934), (26, 0.2423297799076739), (27, 0.3125814392663522)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Apply transformation to the entire corpus and call it 'corpus_tfidf'\n",
    "'''\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "print(corpus_tfidf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.19078924041872244),\n",
      " (1, 0.3511211447021249),\n",
      " (2, 0.17841896683468647),\n",
      " (3, 0.28254428149856087),\n",
      " (4, 0.2427502310046791),\n",
      " (5, 0.2511145439465789),\n",
      " (6, 0.19464950977579026),\n",
      " (7, 0.27509300844171564),\n",
      " (8, 0.16011651229993046),\n",
      " (9, 0.1294591660932731),\n",
      " (10, 0.16574250447273267),\n",
      " (11, 0.32333593060916294),\n",
      " (12, 0.36561089906305133),\n",
      " (13, 0.15198653042920923),\n",
      " (14, 0.3856127405082445),\n",
      " (15, 0.1149605557115203)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview TF-IDF scores for our first document --> --> (token_id, tfidf score)\n",
    "'''\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    \n",
    "    pprint(doc)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Running LDA using Bag of Words ##\n",
    "\n",
    "We are going for 3 topics in the document corpus.\n",
    "\n",
    "Number of requested latent themes to be retrieved from the training corpus is indicated by the variable **num topics**.\n",
    "\n",
    "Word ids (integers) are mapped to words in **id2word** (strings). It is used for topic printing, debugging, and determining the vocabulary size.\n",
    "\n",
    "The quantity of additional processes to use for parallelization is **workers**. use all of the CPU cores by default.\n",
    "\n",
    "The number of training passes through the corpus is **passes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore  2 and 3 ,4,5passes\n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=3, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 4, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.019*\"drive\" + 0.012*\"mile\" + 0.011*\"problem\" + 0.010*\"great\" + 0.010*\"year\" + 0.009*\"buy\" + 0.008*\"replac\" + 0.008*\"truck\" + 0.008*\"like\" + 0.007*\"time\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.058*\"band\" + 0.024*\"great\" + 0.022*\"resist\" + 0.019*\"product\" + 0.016*\"work\" + 0.012*\"exercis\" + 0.012*\"good\" + 0.011*\"handl\" + 0.011*\"qualiti\" + 0.011*\"like\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.017*\"like\" + 0.015*\"good\" + 0.014*\"tast\" + 0.012*\"product\" + 0.012*\"flavor\" + 0.012*\"love\" + 0.010*\"great\" + 0.009*\"coffe\" + 0.008*\"food\" + 0.007*\"chip\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    \n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save lda model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "\n",
    "\n",
    "#saving model to disk.\n",
    "\n",
    "temp_file = datapath(r\"C:\\Users\\60169\\Documents\\Unsupervised-Text-Clustering\\lda_model\")\n",
    "\n",
    "lda_model.save(temp_file)\n",
    "\n",
    "\n",
    "\n",
    "#loading model from disk\n",
    "\n",
    "from gensim import  models\n",
    "\n",
    "lda = models.ldamodel.LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2 Running LDA using TF-IDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define lda model using corpus_tfidf, again using gensim.models.LdaMulticore()\n",
    "'''\n",
    "\n",
    "# 3 and 4 passes\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             num_topics=3, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 5, \n",
    "                                             workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.010*\"love\" + 0.009*\"tast\" + 0.008*\"flavor\" + 0.007*\"coffe\" + 0.007*\"chip\" + 0.006*\"like\" + 0.006*\"food\" + 0.006*\"good\" + 0.005*\"product\" + 0.005*\"chocol\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.010*\"drive\" + 0.008*\"truck\" + 0.007*\"mile\" + 0.006*\"problem\" + 0.005*\"look\" + 0.005*\"year\" + 0.005*\"great\" + 0.005*\"power\" + 0.004*\"vehicl\" + 0.004*\"replac\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.030*\"band\" + 0.015*\"resist\" + 0.013*\"product\" + 0.013*\"great\" + 0.012*\"work\" + 0.010*\"workout\" + 0.010*\"qualiti\" + 0.009*\"exercis\" + 0.008*\"good\" + 0.008*\"easi\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    \n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save lda model TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "#saving model to disk.\n",
    "\n",
    "temp_file1 = datapath(r\"C:\\Users\\60169\\Documents\\Unsupervised-Text-Clustering\\lda_model_tfidf\")\n",
    "\n",
    "lda_model_tfidf.save(temp_file1)\n",
    "\n",
    "\n",
    "\n",
    "#loading model from disk\n",
    "\n",
    "from gensim import  models\n",
    "\n",
    "lda = models.ldamodel.LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "\n",
    "We will check to see where our test document would be classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4310    [corvett, perform, cavali, qualiti, go, mile, ...\n",
       "4310    [regular, purchas, item, bulk, love, dairi, fr...\n",
       "4310                                              [great]\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Text of sample document 4310\n",
    "'''\n",
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9149332046508789\t \n",
      "Topic: 0.019*\"drive\" + 0.012*\"mile\" + 0.011*\"problem\" + 0.010*\"great\" + 0.010*\"year\" + 0.009*\"buy\" + 0.008*\"replac\" + 0.008*\"truck\" + 0.008*\"like\" + 0.007*\"time\"\n",
      "\n",
      "Score: 0.0727204903960228\t \n",
      "Topic: 0.017*\"like\" + 0.015*\"good\" + 0.014*\"tast\" + 0.012*\"product\" + 0.012*\"flavor\" + 0.012*\"love\" + 0.010*\"great\" + 0.009*\"coffe\" + 0.008*\"food\" + 0.007*\"chip\"\n",
      "\n",
      "Score: 0.012346294708549976\t \n",
      "Topic: 0.058*\"band\" + 0.024*\"great\" + 0.022*\"resist\" + 0.019*\"product\" + 0.016*\"work\" + 0.012*\"exercis\" + 0.012*\"good\" + 0.011*\"handl\" + 0.011*\"qualiti\" + 0.011*\"like\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA Bag of Words model.\n",
    "'''\n",
    "document_num = 4310\n",
    "# Our test document is document number 4310\n",
    "\n",
    "for index, score in sorted(lda_model[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It has the highest probability (`x`) to be  part of the topic that we assigned as X, which is the accurate classification. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9743115901947021\t \n",
      "Topic: 0.010*\"drive\" + 0.008*\"truck\" + 0.007*\"mile\" + 0.006*\"problem\" + 0.005*\"look\" + 0.005*\"year\" + 0.005*\"great\" + 0.005*\"power\" + 0.004*\"vehicl\" + 0.004*\"replac\"\n",
      "\n",
      "Score: 0.013053097762167454\t \n",
      "Topic: 0.010*\"love\" + 0.009*\"tast\" + 0.008*\"flavor\" + 0.007*\"coffe\" + 0.007*\"chip\" + 0.006*\"like\" + 0.006*\"food\" + 0.006*\"good\" + 0.005*\"product\" + 0.005*\"chocol\"\n",
      "\n",
      "Score: 0.012635283172130585\t \n",
      "Topic: 0.030*\"band\" + 0.015*\"resist\" + 0.013*\"product\" + 0.013*\"great\" + 0.012*\"work\" + 0.010*\"workout\" + 0.010*\"qualiti\" + 0.009*\"exercis\" + 0.008*\"good\" + 0.008*\"easi\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA TF-IDF model.\n",
    "'''\n",
    "# Our test document is document number 4310\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It has the highest probability (`x%`) to be  part of the topic that we assigned as X. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Testing model on unseen document ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.815620481967926\t Topic: 0.019*\"drive\" + 0.012*\"mile\" + 0.011*\"problem\" + 0.010*\"great\" + 0.010*\"year\"\n",
      "Score: 0.15214847028255463\t Topic: 0.058*\"band\" + 0.024*\"great\" + 0.022*\"resist\" + 0.019*\"product\" + 0.016*\"work\"\n",
      "Score: 0.03223102167248726\t Topic: 0.017*\"like\" + 0.015*\"good\" + 0.014*\"tast\" + 0.012*\"product\" + 0.012*\"flavor\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"My Chevrolet Cavelier is tops in gas milage and would be an excellent car for a young person or old.  However, I think GM needs to improve on the sturdiness of the vehicle.\"\n",
    "\n",
    "# Data preprocessingAbu likes every sport step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8352952003479004\t Topic: 0.017*\"like\" + 0.015*\"good\" + 0.014*\"tast\" + 0.012*\"product\" + 0.012*\"flavor\"\n",
      "Score: 0.1474548876285553\t Topic: 0.058*\"band\" + 0.024*\"great\" + 0.022*\"resist\" + 0.019*\"product\" + 0.016*\"work\"\n",
      "Score: 0.017249947413802147\t Topic: 0.019*\"drive\" + 0.012*\"mile\" + 0.011*\"problem\" + 0.010*\"great\" + 0.010*\"year\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"I've been drinking International Coffee for 35 years now.  My elder sister drank Suisse Mocha when I went to her house, and the instant I hit adulthood, she offered me a cup. I really liked it, but later, my budget did not allow for it, so I tried making my own from a combination of cocoa mix and instant coffee.\"\n",
    "# Data preprocessingAbu likes every sport step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Insurance System (Main Program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will make recommendations to the user based on the comments that they posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "# !pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "import nltk\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import corpora\n",
    "from gensim import  models\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#loading model and dictionary from disk\n",
    "model_file = datapath(r\"C:\\\\Users\\\\60169\\\\Documents\\\\FYP\\\\NoidAI\\\\NoidAI\\\\TopicClustering\\\\lda_model\")\n",
    "dict_file = datapath(r\"C:\\Users\\60169\\Documents\\FYP\\NoidAI\\NoidAI\\TopicClustering\\\\lda_model.id2word\")\n",
    "\n",
    "lda_model = models.ldamodel.LdaModel.load(model_file)\n",
    "dictionary = corpora.Dictionary.load(dict_file)\n",
    "\n",
    "#load lda model tfidf\n",
    "model_file_tfidf = datapath(r\"C:\\\\Users\\\\60169\\\\Documents\\\\FYP\\\\NoidAI\\\\NoidAI\\\\TopicClustering\\\\lda_model_tfidf\")\n",
    "dict_file_tfidf = datapath(r\"C:\\Users\\60169\\Documents\\FYP\\NoidAI\\NoidAI\\TopicClustering\\\\lda_model_tfidf.id2word\")\n",
    "\n",
    "lda_model_tfidf = models.ldamodel.LdaModel.load(model_file_tfidf)\n",
    "dictionary_tfidf = corpora.Dictionary.load(dict_file_tfidf)\n",
    "\n",
    "# NOTE: Path may differ in differeny machines, please configure accordingly.\n",
    "\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        \n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            \n",
    "            # TODO: Apply lemmatize_stemming() on the token, then add to the results list\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_cluster(document):\n",
    "    # Data preprocessingAbu likes every sport step for the unseen document\n",
    "    bow_vector = dictionary.doc2bow(preprocess(document))\n",
    "    cluster = lda_model[bow_vector]\n",
    "    df_cluster = pd.DataFrame(cluster, columns = ['index', 'score'])\n",
    "    max_score = df_cluster.iloc[df_cluster['score'].idxmax()] \n",
    "    document_topic=int(max_score['index'])\n",
    "    \n",
    "    for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
    "    \n",
    "    # topic 0 - car, topic 1 - sports, topic 2 - food\n",
    "    if document_topic == 0:\n",
    "        print(\"\\nThis comment belongs to topic: CAR\\n\\n\")\n",
    "    elif document_topic == 1:\n",
    "        print(\"\\nThis comment belongs to topic: SPORTS\\n\\n\")\n",
    "    else :\n",
    "        print(\"\\nThis comment belongs to topic: FOOD\\n\\n\")\n",
    "        \n",
    "    return document_topic\n",
    "\n",
    "def get_cluster_tfidf(document):\n",
    "    bow_vector_tfidf = dictionary_tfidf.doc2bow(preprocess(document))\n",
    "    cluster_tfidf = lda_model_tfidf[bow_vector_tfidf]\n",
    "    df_cluster_tfidf = pd.DataFrame(cluster_tfidf, columns = ['index', 'score'])\n",
    "    max_score_tfidf = df_cluster_tfidf.iloc[df_cluster_tfidf['score'].idxmax()] \n",
    "    topic_tfidf=int(max_score_tfidf['index'])\n",
    "    \n",
    "    for index, score in sorted(lda_model_tfidf[bow_vector_tfidf], key=lambda tup: -1*tup[1]):\n",
    "        print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))\n",
    "    \n",
    "    # topic 0: Food, topic 1: Car, topic 2: Sports\n",
    "    if topic_tfidf == 0:\n",
    "        print(\"\\nThis comment belongs to topic: FOOD\")\n",
    "    elif topic_tfidf == 1:\n",
    "        print(\"\\nThis comment belongs to topic: CAR\")\n",
    "    else :\n",
    "        print(\"\\nThis comment belongs to topic: SPORTS\")\n",
    "        \n",
    "    return topic_tfidf\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Insurance Products\n",
    "# 0 - Life Insurance\n",
    "# 1 - Health Insurance\n",
    "# 2 - Car Insurance\n",
    "# 3 - Accident Insurance\n",
    "insurance_product = ['Life Insurance','Health Insurance','Car Insurance', 'Accident Insurance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Machine Learning Based Algorithm in Identifying Potential Insurance Customer in Social Media with Text\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "1) Input Text\n",
      "Q) Exit\n",
      "\n",
      "Enter your choice: 1\n",
      "\n",
      "Please enter a comment:\n",
      "Weights with the Junior Youth Academy , who doesn’t like learning how to hold a bar :)\n",
      "\n",
      "LDA Bag of Words model\n",
      "-------------------\n",
      "Score: 0.8499441742897034\t Topic: 0.058*\"band\" + 0.024*\"great\" + 0.022*\"resist\" + 0.019*\"product\" + 0.016*\"work\"\n",
      "Score: 0.0765596255660057\t Topic: 0.017*\"like\" + 0.015*\"good\" + 0.014*\"tast\" + 0.012*\"product\" + 0.012*\"flavor\"\n",
      "Score: 0.07349622994661331\t Topic: 0.019*\"drive\" + 0.012*\"mile\" + 0.011*\"problem\" + 0.010*\"great\" + 0.010*\"year\"\n",
      "\n",
      "This comment belongs to topic: SPORTS\n",
      "\n",
      "\n",
      "LDA TF-IDF model\n",
      "-------------------\n",
      "Score: 0.8528741002082825\t Topic: 0.030*\"band\" + 0.015*\"resist\" + 0.013*\"product\" + 0.013*\"great\" + 0.012*\"work\"\n",
      "Score: 0.07384490221738815\t Topic: 0.010*\"love\" + 0.009*\"tast\" + 0.008*\"flavor\" + 0.007*\"coffe\" + 0.007*\"chip\"\n",
      "Score: 0.07328102737665176\t Topic: 0.010*\"drive\" + 0.008*\"truck\" + 0.007*\"mile\" + 0.006*\"problem\" + 0.005*\"look\"\n",
      "\n",
      "This comment belongs to topic: SPORTS\n",
      "\n",
      "                              Insurance Recommendation \n",
      "--------------------------------------------------------------------------------\n",
      "Based on the comment you posted, we recommend you purchase :\n",
      "1. Life Insurance\n",
      "2. Accident Insurance\n",
      "\n",
      "Machine Learning Based Algorithm in Identifying Potential Insurance Customer in Social Media with Text\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "1) Input Text\n",
      "Q) Exit\n",
      "\n",
      "Enter your choice: q\n"
     ]
    }
   ],
   "source": [
    "def show_menu():\n",
    "    print (\"\\nMachine Learning Based Algorithm in Identifying Potential Insurance Customer in Social Media with Text\")\n",
    "    print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "    print (\"1) Input Text\")\n",
    "    print (\"Q) Exit\\n\")\n",
    " \n",
    "def menu():\n",
    "    while True:\n",
    "        show_menu()\n",
    "        choice = input('Enter your choice: ').lower()\n",
    "        comment_recommendation = []\n",
    "        if choice == '1':\n",
    "            comment = input(\"\\nPlease enter a comment:\\n\")\n",
    "            print(\"\\nLDA Bag of Words model\")\n",
    "            print(\"-------------------\")\n",
    "            get_cluster(comment)\n",
    "            print(\"LDA TF-IDF model\")\n",
    "            print(\"-------------------\")\n",
    "            comment_cluster = get_cluster_tfidf(comment)\n",
    "            # Recommender System based on text\n",
    "            # Car person\n",
    "            if comment_cluster == 1:\n",
    "                comment_recommendation.append(insurance_product[0])\n",
    "                comment_recommendation.append(insurance_product[2])\n",
    "                comment_recommendation.append(insurance_product[3])\n",
    "            # Exercise person\n",
    "            elif comment_cluster == 2:\n",
    "                comment_recommendation.append(insurance_product[0])\n",
    "                comment_recommendation.append(insurance_product[3])\n",
    "            else :\n",
    "                comment_recommendation.append(insurance_product[1]) \n",
    "            # Display the Recommendation\n",
    "            print(\"\\n                              Insurance Recommendation \")\n",
    "            print(\"--------------------------------------------------------------------------------\")\n",
    "            if len(comment_recommendation) != 0:\n",
    "                print(\"Based on the comment you posted, we recommend you purchase :\")\n",
    "                cmm_rec_str = ''.join(comment_recommendation)\n",
    "                cmm_rec_str\n",
    "                \n",
    "                for i in range(0,len(comment_recommendation), 1):\n",
    "                    print('{}. {}'.format(i+1, comment_recommendation[i]))\n",
    "                    \n",
    "            time.sleep(5)\n",
    "        elif choice == 'q':\n",
    "            return\n",
    "        else:\n",
    "            print(f'Not a correct choice: <{choice}>,try again')\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    import cv2\n",
    "    import numpy as np\n",
    " \n",
    "    \n",
    "    menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
